---
title: COVID sample analysis (comparison of variable regions)
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
  pdf_document:
    toc: yes
fig_width: 10
fig_height: 10
---

```{R, results='hide', fig.keep='all', message=FALSE, include=FALSE}
library(reticulate)
library(kableExtra)
library(knitr)
# library(exactRankTests)
# library(nlme)
# library(dplyr)
# library(ggplot2)
# library(compositions)
# library(vegan)
# library(phyloseq)
```

```{python, results='hide', fig.keep='all', message=FALSE, include=FALSE}
import numpy as np
import os
import pandas as pd
import math
import matplotlib.pyplot as plt
from scipy.cluster import hierarchy
import matplotlib as mpl
from matplotlib_venn import venn2
import csv
from matplotlib.patches import Patch
import pickle
from scipy.spatial import distance
from scipy import stats
from sklearn import manifold
from sklearn.decomposition import PCA
from scipy.stats import pearsonr, spearmanr

folder = '/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/COVID/'
```

# Sequence processing {.tabset}

## Amplicon {.tabset}

### Activate QIIME2
Initially used some combination of both 2020.2 and 2020.8, so re-doing all with 2020.8 (not reimporting reads, starting by using the existing reads.qza object)
```{bash, eval=FALSE}
conda activate qiime2-2020.8
```

### Initial quality checks
```{bash, eval=FALSE}
mkdir fastqc
#fastqc -t 4 COVID-TNA-16S_Run380/*fastq.gz -o fastqc/
fastqc -t 4 reads/*fastq.gz -o fastqc/
multiqc fastqc/
```

### Import and summarize
```{bash, eval=FALSE}
#qiime tools import \
#  --type SampleData[PairedEndSequencesWithQuality] \
#  --input-path COVID-TNA-16S_Run380/ \
#  --output-path reads.qza \
#  --input-format CasavaOneEightSingleLanePerSampleDirFmt

qiime tools import \
  --type SampleData[PairedEndSequencesWithQuality] \
  --input-path reads/ \
  --output-path reads.qza \
  --input-format CasavaOneEightSingleLanePerSampleDirFmt
  
qiime demux summarize \
  --i-data reads.qza  \
  --o-visualization summary_reads.qzv
```

### Cutadapt V4V5
```{bash, eval=FALSE}
qiime cutadapt trim-paired \
  --i-demultiplexed-sequences reads.qza \
  --p-cores 8 \
  --p-front-f GTGYCAGCMGCCGCGGTAA \
  --p-front-r CCGYCAATTYMTTTRAGTTT \
  --p-discard-untrimmed \
  --p-no-indels \
  --o-trimmed-sequences trimmed_reads.qza
  
qiime demux summarize \
  --i-data trimmed_reads.qza  \
  --o-visualization summary_trimmed_reads.qzv
```

### Cutadapt V6V8
```{bash, eval=FALSE}
qiime cutadapt trim-paired \
  --i-demultiplexed-sequences reads.qza \
  --p-cores 12 \
  --p-front-f ACGCGHNRAACCTTACC \
  --p-front-r ACGGGCRGTGWGTRCAA \
  --p-discard-untrimmed \
  --p-no-indels \
  --o-trimmed-sequences trimmed_reads.qza
  
qiime demux summarize \
  --i-data trimmed_reads.qza  \
  --o-visualization summary_trimmed_reads.qzv
```

### DADA2
```{bash, eval=FALSE}
mkdir dada2_out
qiime dada2 denoise-paired \
  --i-demultiplexed-seqs trimmed_reads.qza \
  --p-trunc-len-f 240 \
  --p-trunc-len-r 180 \
  --p-max-ee-f 2 \
  --p-max-ee-r 2 \
  --p-n-threads 12 \
  --o-table dada2_out/table.qza \
  --o-representative-sequences dada2_out/representative_sequences.qza \
  --o-denoising-stats dada2_out/stats.qza
  
qiime metadata tabulate \
  --m-input-file dada2_out/stats.qza \
  --o-visualization stats_dada2_out.qzv
  
qiime feature-table summarize \
  --i-table dada2_out/table.qza  \
  --o-visualization summary_dada2_out.qzv
```

### Classify
```{bash, eval=FALSE}
#wget https://data.qiime2.org/2020.8/common/silva-138-99-nb-classifier.qza
qiime feature-classifier classify-sklearn \
  --i-reads dada2_out/representative_sequences.qza \
  --i-classifier /home/shared/taxa_classifiers/qiime2-2020.8_classifiers/silva-138-99-nb-classifier.qza \
  --p-n-jobs 12 \
  --output-dir taxa
  
qiime tools export \
  --input-path taxa/classification.qza \
  --output-path taxa
```

### Filter features
```{bash, eval=FALSE}
qiime feature-table filter-features \
  --i-table dada2_out/table.qza \
  --p-min-frequency 5 \
  --p-min-samples 1 \
  --o-filtered-table table_filtered.qza
  
qiime feature-table summarize \
  --i-table table_filtered.qza \
  --o-visualization summary_table_filtered.qzv
```

```{bash, eval=FALSE}
qiime taxa filter-table \
  --i-table table_filtered.qza \
  --i-taxonomy taxa/classification.qza \
  --p-include d__ \
  --p-exclude mitochondria,chloroplast \
  --o-filtered-table table_filtered_contamination.qza
  
qiime feature-table summarize \
  --i-table table_filtered_contamination.qza \
  --o-visualization summary_table_filtered_contamination.qzv
```

### Get rarefaction curves
V4V5
```{bash, eval=FALSE}
qiime diversity alpha-rarefaction \
  --i-table table_filtered_contamination.qza \
  --p-max-depth 22840 \
  --p-steps 20 \
  --p-metrics 'observed_features' \
  --o-visualization rarefaction_curves.qzv
```

cDNA V6V8
```{bash, eval=FALSE}
qiime diversity alpha-rarefaction \
  --i-table table_filtered_contamination.qza \
  --p-max-depth 160269 \
  --p-steps 20 \
  --p-metrics 'observed_features' \
  --o-visualization rarefaction_curves.qzv
```

TNA V6V8
```{bash, eval=FALSE}
qiime diversity alpha-rarefaction \
  --i-table table_filtered_contamination.qza \
  --p-max-depth 464496 \
  --p-steps 20 \
  --p-metrics 'observed_features' \
  --o-visualization rarefaction_curves.qzv
```

### Filter
*Not rarefying or removing samples with low numbers of reads*
```{bash, eval=FALSE}
qiime feature-table filter-seqs \
  --i-data dada2_out/representative_sequences.qza \
  --i-table table_filtered_contamination.qza \
  --o-filtered-data  representative_sequences_filtered_contamination.qza
```

### Insert sequences into tree
```{bash, eval=FALSE}
qiime fragment-insertion sepp \
  --i-representative-sequences representative_sequences_filtered_contamination.qza \
  --i-reference-database /home/robyn/tools/sepp/sepp-refs-silva-128.qza \
  --o-tree insertion_tree.qza \
  --o-placements insertion_placements.qza \
  --p-threads 12
```

### Export all files
```{bash, eval=FALSE}
qiime tools export \
  --input-path representative_sequences_filtered_contamination.qza \
  --output-path exports
  
sed -i -e '1 s/Feature/#Feature/' -e '1 s/Taxon/taxonomy/' taxa/taxonomy.tsv

qiime tools export \
  --input-path table_filtered_contamination.qza \
  --output-path exports
  
biom add-metadata \
  -i exports/feature-table.biom \
  -o exports/feature-table_w_tax.biom \
  --observation-metadata-fp taxa/taxonomy.tsv \
  --sc-separated taxonomy
  
biom convert \
  -i exports/feature-table_w_tax.biom \
  -o exports/feature-table_w_tax.txt \
  --to-tsv \
  --header-key taxonomy
  
qiime tools export \
  --input-path insertion_tree.qza \
  --output-path exports
```

```{bash, eval=FALSE}
qiime taxa barplot \
  --i-table table_filtered_contamination.qza \
  --i-taxonomy taxa/classification.qza \
  --m-metadata-file metadata.txt \
  --o-visualization barplot.qzv
```

## Metagenome kneaddata/Kraken2 {.tabset}

### Concatenate lanes
```{bash, eval=FALSE}
concat_lanes.pl COVID-TNA_cDNA-MetaG_RunNS56/* -o cat_lanes -p 4
```

### Kneaddata
```{bash, eval=FALSE}
parallel -j 2 --link 'kneaddata -i {1} -i {2} -o kneaddata_out/ \
-db /home/shared/bowtiedb/GRCh38_PhiX --trimmomatic /home/robyn/tools/Trimmomatic-0.39/ \
-t 20 --trimmomatic-options "SLIDINGWINDOW:4:20 MINLEN:50" \
--bowtie2-options "--very-sensitive --dovetail" --remove-intermediate-output' \
 ::: cat_lanes/*_R1.fastq.gz ::: cat_lanes/*_R2.fastq.gz
 
kneaddata_read_count_table --input kneaddata_out --output kneaddata_read_counts.txt
```

### Concatenate reads
```{bach, eval=FALSE}
concat_paired_end.pl -p 4 --no_R_match -o cat_reads kneaddata_out/*_paired_*.fastq 
```

### Kraken2 (RefSeq Complete v93)
```{bash, eval=FALSE}
mkdir kraken2_outraw_conf
mkdir kraken2_kreport_conf
parallel -j 2 'kraken2 --use-names --threads 12 --db /scratch/ramdisk/Kraken2.0.8_Bracken150mer_RefSeqCompleteV93/ --memory-mapping {} --output kraken2_outraw/{/.}.kraken.txt --report kraken2_kreport/{/.}.kreport --confidence 0.1' ::: cat_reads/*.fastq
```

### Bracken
```{bash, eval=FALSE}
parallel -j 1 'bracken -d /scratch/ramdisk/Kraken2.0.8_Bracken150mer_RefSeqCompleteV93/ -i {} -l S -o {.}.bracken -r 150' ::: kraken2_kreport/*.kreport

parallel -j 1 'bracken -d /scratch/ramdisk/Kraken2.0.8_Bracken150mer_RefSeqCompleteV93/ -i {} -l S -o {.}.bracken -r 150' ::: kraken2_kreport/O3-pos.kreport
```

### Kraken2 (Standard 19-9-2020 incl. SARS-COV-2)
```{bash, eval=FALSE}
mkdir kraken_standard/kraken2_outraw
mkdir kraken_standard/kraken2_kreport
parallel -j 1 'kraken2 --use-names --threads 12 --db /home/robyn/tools/kraken_db_Nov20/ --memory-mapping {} --output kraken_standard_Nov20/kraken2_outraw/{/.}.kraken.txt --report kraken_standard_Nov20/kraken2_kreport/{/.}.kreport --confidence 0.1' ::: cat_reads/*.fastq
```

### Bracken
```{bash, eval=FALSE}
parallel -j 1 'bracken -d /home/robyn/tools/kraken_db_Nov20/ -i {} -l S -o {.}.bracken -r 150' ::: kraken_standard/kraken2_kreport/*.kreport

#parallel -j 1 'bracken -d /home/robyn/tools/kraken_db_Nov20/ -i {} -l S -o {.}.bracken -r 150' ::: kraken2_kreport/O3-pos.kreport
```

### One more sample

O3-pos didn't run the first time and kraken seemed to have some kind of an issue with the file. I've converted it to fasta so hoping this works.

```{bash, eval=FALSE}
parallel -j 1 'kraken2 --use-names --threads 12 --db /home/robyn/tools/kraken_db_Nov20/ --memory-mapping {} --output kraken_standard_Nov20/kraken2_outraw/{/.}.kraken.txt --report kraken_standard_Nov20/kraken2_kreport/{/.}.kreport --confidence 0.1' ::: cat_reads/*.fasta

parallel -j 1 'bracken -d /home/robyn/tools/kraken_db_Nov20/ -i {} -l S -o {.}.bracken -r 150' ::: kraken_standard_Nov20/kraken2_kreport/O3-pos.kreport

parallel -j 1 'kraken2 --use-names --threads 12 --db /scratch/ramdisk/Kraken2.0.8_Bracken150mer_RefSeqCompleteV93/ --memory-mapping {} --output kraken_RSCv93/kraken2_outraw/{/.}.kraken.txt --report kraken_RSCv93/kraken2_kreport/{/.}.kreport --confidence 0.1' ::: cat_reads/*.fasta

parallel -j 1 'bracken -d /scratch/ramdisk/Kraken2.0.8_Bracken150mer_RefSeqCompleteV93/ -i {} -l S -o {.}.bracken -r 150' ::: kraken_RSCv93/kraken2_kreport/O3-pos.kreport
```

### Run with different confidence

Standard:
```{bash, eval=FALSE}
parallel -j 1 'kraken2 --use-names --threads 12 --db /home/robyn/tools/kraken_db_Nov20/ --memory-mapping {1} --output kraken_standard_Nov20/kraken2_outraw/{1/.}.kraken.txt --report kraken_standard_Nov20/kraken2_kreport/{1/.}_{2}.kreport --confidence {2}' ::: cat_reads/* ::: 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

parallel -j 1 'bracken -d /home/robyn/tools/kraken_db_Nov20/ -i {} -l S -o {.}.bracken -r 150' ::: kraken_standard_Nov20/kraken2_kreport/*
```

RSC v93:
```{bash, eval=FALSE}
parallel -j 1 'kraken2 --use-names --threads 12 --db /scratch/ramdisk/Kraken2.0.8_Bracken150mer_RefSeqCompleteV93/ --memory-mapping {1} --output kraken_RSCv93/kraken2_outraw/{1/.}_{2}.kraken.txt --report kraken_RSCv93/kraken2_kreport/{1/.}_{2}.kreport --confidence {2}' ::: cat_reads/* ::: 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

parallel -j 1 'bracken -d /scratch/ramdisk/Kraken2.0.8_Bracken150mer_RefSeqCompleteV93/ -i {} -l S -o {.}.bracken -r 150' ::: kraken_RSCv93/kraken2_kreport/*
```

## Reads mapping to SARS-CoV-2 genome {.tabset}

### Kneaddata/Bowtie2

Make SARS-COV-2 database:
```{bash, eval=FALSE}
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/009/858/895/GCF_009858895.2_ASM985889v3/GCF_009858895.2_ASM985889v3_genomic.fna.gz
bowtie2-build GCF_009858895.2_ASM985889v3_genomic.fna.gz SARS-COV-2
```

Run with the cat_reads MG samples:
```{bash, eval=FALSE}
parallel -j 1 --link --progress 'kneaddata -i {1} -i {2} -o kneaddata_out_covid/ \
-db covid_ref_genome/ --trimmomatic /home/robyn/tools/Trimmomatic-0.39/ \
-t 12 --trimmomatic-options "SLIDINGWINDOW:4:20 MINLEN:50" \
--bowtie2-options "--very-sensitive --dovetail" --remove-intermediate-output' \
 ::: cat_lanes/*_R1.fastq.gz ::: cat_lanes/*_R2.fastq.gz
 
kneaddata_read_count_table --input kneaddata_out_covid/ --output kneaddata_read_counts_covid_genome.txt
```

Resulting sample sizes:
```{python, results='hide', fig.keep='all'}
import pandas as pd
import matplotlib.pyplot as plt

samples = pd.read_csv('/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/COVID/kneaddata_read_counts_covid_genome.txt', sep='\t', header=0, index_col=0)
print(samples)
nonzero = {'cDNA-N1-pos_S75_R1_kneaddata':103, 'cDNA-N2-pos_S76_R1_kneaddata':130, 'cDNA-N3-pos_S77_R1_kneaddata':1.1, 'cDNA-N4-pos_S78_R1_kneaddata':6.9, 'cDNA-N5-pos_S79_R1_kneaddata':20, 'cDNA-O1-pos_S80_R1_kneaddata':1.5, 'cDNA-O2-pos_S61_R1_kneaddata':118, 'cDNA-O3-pos_S62_R1_kneaddata':23, 'cDNA-O4-pos_S63_R1_kneaddata':3.4, 'cDNA-O5-pos_S64_R1_kneaddata':15}

plt.figure(figsize=(10,3))
ax1 = plt.subplot(111)

count = 0
names, x = [], []
for sample in samples.index.values:
  if sample in nonzero:
    ax1.bar(count, nonzero[sample], color='k')
  x.append(count)
  names.append(sample.split('_')[0])
  count += 1

plt.xticks(x, names, rotation=90)
plt.ylabel('File size (Kb)')
plt.tight_layout()
plt.show()
```

### metaSPADES {.tabset}

#### Test

Needs single short-read library, with paired-end reads

Install:
```{bash, eval=FALSE}
#Check dependencies
g++ --version
g++ (Ubuntu 5.5.0-12ubuntu1~16.04) 5.5.0 20171010
cmake --version
cmake version 3.5.1
conda install zlib #still isn't showing with which or --version, but it says it installed so I'll try the rest anyway.
conda install libbz2 #also didn't install, so trying anyway

wget http://cab.spbu.ru/files/release3.14.1/SPAdes-3.14.1.tar.gz
tar -xzf SPAdes-3.14.1.tar.gz
cd SPAdes-3.14.1

PREFIX=/home/robyn/anaconda3/ ./spades_compile.sh
```

Test:
```{bash, eval=FALSE}
/home/robyn/anaconda3/bin/spades.py --test
```

Input:
- paired end reads (using fastq files from concatenating lanes)

Test single sample:
```{bash, eval=FALSE}
/home/robyn/anaconda3/bin/spades.py -o test_spades_out --meta -1 cat_lanes/cDNA-N1-pos_S75_R1.fastq.gz -2 cat_lanes/cDNA-N1-pos_S75_R2.fastq.gz -t 12
```

Run:
```{bash, eval=FALSE}
/home/robyn/anaconda3/bin/spades.py -o test_spades_out --meta -1 cat_lanes/cDNA-N1-pos_S75_R1.fastq.gz -2 cat_lanes/cDNA-N1-pos_S75_R2.fastq.gz -t 12
```

Took ~5 hours to run using 12 threads on vulcan.

Install MetaQUAST:
```{bash, eval=FALSE}
wget https://downloads.sourceforge.net/project/quast/quast-5.0.2.tar.gz
tar -xzf quast-5.0.2.tar.gz
cd quast-5.0.2
```
Seems to work fine.

Run on the sample:
```{bash, eval=FALSE}
python /home/robyn/tools/quast-5.0.2/metaquast.py test_spades_out/contigs.fasta --references-list reference-list.txt --threads 10
```

This gives the following COVID genome alignment graph:
![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/COVID/metagenome/Icarus_test.png)
The largest contig that aligned was 2747 and the total aligned length was 9760 (32.6% COVID genome coverage).

#### All samples

Pipeline used in [Virome characterisation comparison paper](https://microbiomejournal.biomedcentral.com/articles/10.1186/s40168-019-0626-5):</br>
1. Trimmomatic</br>
2. MetaSPAdes</br>
3. MetaQUAST</br></br>

Because MetaSPAdes took quite a long time to run with the whole sample, I'm going to do this first with "contam" output from Bowtie2 above, so those reads that mapped to the COVID genome (i.e. skipping the Trimmomatic step for not). </br></br>

**MetaSPAdes:**
```{bash, eval=FALSE}
parallel -j 1 --link --progress '/home/robyn/anaconda3/bin/spades.py --meta -1 {1} -2 {2} -t 12 -o metaSPADES/{1/.}' ::: kneaddata_covid_contam/*_1.fastq ::: kneaddata_covid_contam/*_2.fastq
```

**MetaQUAST:**
```{bash, eval=FALSE}
parallel -j 1 --link --progress 'python /home/robyn/tools/quast-5.0.2/metaquast.py {1}/contigs.fasta --references-list reference-list.txt --threads 10 -o quast_results/{1/}' ::: metaSPADES/*
```

#### Alignment summary

```{python, results='hide', fig.keep='all'}
samples = ['N1', 'N1', 'N2', 'N4', 'N5', 'O2', 'O3', 'O4', 'O5']
largest_aligned_contig = [2747, 2721, 4016, 0, 4745, 4743, 502, 0, 0]
total_aligned = [9760, 8536, 17872, 0, 26891, 12041, 502, 0, 0]
total_perc_aligned = [32.6, 28.519, 59.8, 0, 89.7, 40.299, 1.68, 0, 0]
colors = ['w', 'r', 'r', 'r', 'r', 'b', 'b', 'b', 'b']
label = ['Largest aligned contig (bp)', 'Total aligned (bp)', 'COVID genome coverage (%)']

plt.figure(figsize=(10,4))
ax1, ax2, ax3 = plt.subplot(131), plt.subplot(132), plt.subplot(133)
ax = [ax1, ax2, ax3]
x = []
for a in range(len(samples)):
  ax1.bar(a, largest_aligned_contig[a], color=colors[a], width=0.8, edgecolor='k')
  ax2.bar(a, total_aligned[a], color=colors[a], width=0.8, edgecolor='k')
  ax3.bar(a, total_perc_aligned[a], color=colors[a], width=0.8, edgecolor='k')
  x.append(a)
for a in range(len(ax)):
  plt.sca(ax[a])
  plt.xticks(x, samples)
  plt.ylabel(label[a])
  
plt.tight_layout()
plt.show()
```
Note that the white N1 bar is for the test above (i.e. the whole sample was assembled and aligned to the COVID genome) while the red N1 bar is for only the reads that bowtie2 aligned to the COVID genome. For reference, the COVID genome is 29,882 bp.

So the sample with the highest % aligned is N5:
![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/COVID/metagenome/quast/quast_results/cDNA-N5-pos_S79_R1_kneaddata_SARS-COV-2_bowtie2_paired_contam_1/icarus_viewers/alignment.png)

## Coverage of genomes SPAdes/QUAST {.tabset}

### Get genome IDs of taxonomic matches

Currently we have used Kraken2 with two databases: 
- RefSeq Complete v93
- Standard databases from Nov 20 (includes SARS-CoV-2 genome)

Get taxid's for all genomes that we want. At the moment I'm not getting any genomes that are from eukaryotes, but this could obviously be changed, and we're keeping only taxa that are above 0.1% relative abundance in the reads after eukaryotic reads are removed:
This script takes the [genbank assembly summary](ftp://ftp.ncbi.nlm.nih.gov/genomes/ASSEMBLY_REPORTS/) (assembly_summary_genbank.txt), [merged taxid list](https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/) (taxdmp.zip / taxdmp.tar.gz), the folder containing the bracken .bracken and bracken.kreport files and a folder + name that should be used to save the resulting download paths list.
```{python, eval=FALSE}
name = 'kraken_RSCV93'
folder = '/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/COVID/metagenome/'+name+'/kraken2_kreport/'
save_folder = '/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/COVID/metagenome/'
ncbi_assembly_fn = '/Users/robynwright/Downloads/assembly_summary_genbank.txt'
ncbi_merged_fn = '/Users/robynwright/Downloads/taxdump/merged.dmp'

bracken = os.listdir(folder)
bracken = [f for f in bracken if '.bracken' in f]
kreport = os.listdir(folder)
kreport = [f for f in kreport if 'bracken.kreport' in f]

keeping_taxid = []
domain = ''
# taxid_domain_dict = {}
for krep in kreport:
  fn = krep
  krep = pd.read_csv(folder+krep, names=['perc', 'reads', 'reads2', 'level', 'classification'], index_col=4, sep='\t')
  for row in krep.index.values:
    if krep.loc[row, 'level'] == 'D':
      domain = krep.loc[row, 'classification']
      domain = domain.replace(' ', '')
    if krep.loc[row, 'level'] == 'S':
      # taxid_domain_dict[row] = domain
      if domain != 'Eukaryota':
        keeping_taxid.append(row)
keeping_taxid = set(keeping_taxid)

taxid = []

for brack in bracken:
  fn = brack
  brack = pd.read_csv(folder+brack, header=0, index_col=1, sep='\t')
  keeping = []
  for row in set(brack.index.values):
    if row in keeping_taxid:
      keeping.append(row)
  brack = brack.loc[keeping, :]
  reads_remain = (sum(brack.loc[:, 'new_est_reads'])/100)*0.1
  brack = brack[brack['new_est_reads'] > reads_remain]
  taxid = list(taxid)+list(brack.index.values)
  taxid = set(taxid)

ncbi_assembly = pd.read_csv(ncbi_assembly_fn, header=1, index_col=6, sep='\t')
ncbi_assembly = ncbi_assembly.reset_index().drop_duplicates(subset='species_taxid', keep='first').set_index('species_taxid')
ncbi_assembly2 = pd.read_csv(ncbi_assembly_fn, header=1, index_col=5, sep='\t')
ncbi_assembly2 = ncbi_assembly2.reset_index().drop_duplicates(subset='taxid', keep='first').set_index('taxid')
merged = pd.read_csv(ncbi_merged_fn, index_col=0, names=['line1', 'new_taxid', 'line2'], sep='\t')

not_in_assembly = []
taxid_ftp = {}
merged_tax = {}
for tax in taxid:
  assembly_using = ncbi_assembly
  if tax not in set(ncbi_assembly.index.values):
    if tax in set(ncbi_assembly2.index.values):
      assembly_using = ncbi_assembly2
    else:
      if tax in set(merged.index.values):
        new_tax = merged.loc[tax, 'new_taxid']
        merged_tax[new_tax] = tax
        tax = new_tax
      else:
        not_in_assembly.append(tax)
        continue
  ftp = assembly_using.loc[tax, 'ftp_path']
  ftp = ftp+'/'+ftp.split('/')[-1]+'_genomic.fna.gz'
  taxid_ftp[tax] = ftp

with open(save_folder+name+'.dict', 'wb') as f:
    pickle.dump(taxid_ftp, f)
```

Now download all of the genomes (they will be named by their taxid):
```{python, eval=FALSE}
import os
import pickle

#folder, file = 'kraken_RSCV93/', 'kraken_RSCV93.dict'
folder, file = 'kraken_standard_Nov20/', 'kraken_standard_Nov20.dict'

with open(file, 'rb') as f:
    taxid_ftp = pickle.load(f)

for ftp in taxid_ftp:
  command = 'wget '+taxid_ftp[ftp]+' -O '+folder+str(ftp)+'.fna.gz'
  os.system(command)
```

### Assemble (SPAdes)

Copied all (kneaddata output) across to Compute Canada server.
Script to make jobs:
```{python, eval=FALSE}
import os 

files = sorted(os.listdir('kneaddata_out_paired/'))
fns, pairs = [], []
for f in range(len(files)):
  fn = files[f].split('_')[0]
  if fn not in fns:
    fns.append(fn)
    for g in range(len(files)):
      if f != g:
        gn = files[g].split('_')[0]
        if fn == gn:
          pairs.append([files[f], files[g]])

for a in range(len(fns)):
  fn = fns[a]
  if fn == 'cDNA-N1-neg': continue
  script = '#!/bin/bash\n'
  script += '#SBATCH --job-name='+fn+'_spades.job\n'
  script += '#SBATCH --output=/home/rwright/scratch/out/'+fn+'_spades.out\n'
  script += '#SBATCH --error=/home/rwright/scratch/out/'+fn+'_spades.err\n'
  script += '#SBATCH --mem=80G\n'
  script += '#SBATCH --time=0-1:30\n'
  script += '#SBATCH --cpus-per-task=40\n'
  script += '#SBATCH --mail-user=robyn.wright@dal.ca\n'
  script += '#SBATCH --mail-type=ALL\n'
  script += 'conda activate spades\n'
  script += 'source activate spades\n'
  script += 'mkdir spades_out/'+fn+'\n'
  script += '/home/rwright/anaconda3/bin/spades.py --meta -1 /home/rwright/scratch/COVID/kneaddata_out_paired/'+pairs[a][0]+' -2 /home/rwright/scratch/COVID/kneaddata_out_paired/'+pairs[a][1]+' -o spades_out/'+fn+'/ -t 40\n'
  with open(fn+'.job', 'w') as f:
    f.write(script)
  os.system('sbatch '+fn+'.job')
```

Copied all back to Vulcan and get only the contigs files:
```{python, eval=FALSE}
import os

samples = os.listdir('spades_out/')
for sample in samples:
  command = 'cp spades_out/'+sample+'/contigs.fasta spades_contigs/'+sample+'_contigs.fasta'
  os.system(command)

```

### MetaQUAST

Do separately for each set of genomes.

```{bash, eval=FALSE}
python /home/robyn/tools/quast-5.0.2/metaquast.py spades_contigs/* -r whole_genomes_NCBI/kraken_RSCV93/* --threads 12 -o quast_results/COVID_MG_kraken_RSCV93_100

python /home/robyn/tools/quast-5.0.2/metaquast.py spades_contigs/* -r whole_genomes_NCBI/kraken_standard_Nov20/* --threads 12 -o quast_results/COVID_MG_kraken_standard_Nov20_100
```

With minimum contig size set to 100 bp:
```{bash, eval=FALSE}
python /home/robyn/tools/quast-5.0.2/metaquast.py spades_contigs/* -r whole_genomes_NCBI/kraken_RSCV93/* --threads 12 -o quast_results/COVID_MG_kraken_RSCV93_100 --min-contig 100

python /home/robyn/tools/quast-5.0.2/metaquast.py spades_contigs/* -r whole_genomes_NCBI/kraken_standard_Nov20/* --threads 12 -o quast_results/COVID_MG_kraken_standard_Nov20_100 --min-contig 100
```

Separately for each sample:
```{bash, eval=FALSE}
parallel -j 1 --link 'python /home/robyn/tools/quast-5.0.2/metaquast.py {} -r whole_genomes_NCBI/kraken_standard_Nov20/* --threads 12 -o quast_results/separate_kraken/{/.}_COVID_MG_kraken_standard_Nov20_100 --min-contig 100' ::: spades_contigs/*
 
parallel -j 3 --link 'python /home/robyn/tools/quast-5.0.2/metaquast.py {} -r whole_genomes_NCBI/kraken_RSCV93/* --threads 12 -o quast_results/separate_kraken/{/.}_COVID_MG_kraken_RSCV93_100 --min-contig 100' ::: spades_contigs/*
```

Separately for each sample with reads not contigs</br>
Single sample test:
```{bash, eval=FALSE}
(/usr/bin/time -v python /home/robyn/tools/quast-5.0.2/metaquast.py cat_reads/cDNA-N1-neg.fasta -r whole_genomes_NCBI/kraken_RSCV93/* --threads 12 -o quast_results/reads_cDNA-N1-neg --min-contig 100) 2> cDNA-N1-neg_quast_reads_time.txt
```

Get only summaries and reports for each sample:
```{python, eval=FALSE}
import os

folder = 'separate_kraken_standard/'
new_fol = 'summaries_separate_kraken/'
samples = os.listdir(folder)

for sample in samples:
  #rep = '/report.html'
  #frac = '/summary/TSV/Genome_fraction.tsv'
  #la = '/summary/TSV/Largest_alignment.tsv'
  #lc = '/summary/TSV/Largest_contig.tsv'
  #nc = '/summary/TSV/num_contigs.tsv'
  ta = '/summary/TSV/Total_aligned_length.tsv'
  to_copy = [ta]#rep, frac, la, lc, nc]
  for copy in to_copy:
    string = 'cp '+folder+sample+copy+' '+new_fol+sample+'_'+copy.split('/')[-1]
    os.system(string)
```

## Generating MAGs Anv'io {.tabset}

## Nonpareil 3

Install (using Source Code Installation [here](https://nonpareil.readthedocs.io/en/latest/installation.html)):
```{bash, eval=FALSE}
git clone git://github.com/lmrodriguezr/nonpareil.git
cd nonpareil
make
make prefix=/home/vinko/tools install
```

Skipped preprocessing the reads (this would just be converting from fasta to fastq)</br>
Run single sample:
```{bash, eval=FALSE}
nonpareil -s cat_reads/cDNA-N1-neg.fastq -T kmer -f fastq -b output -X 100
```
The only thing to be careful of is that -X (number of samples to take) must be at least 100 times less than the number of reads. So in the following script I'm counting the number of reads in a file and dividing that by 100 before starting. If this is bigger than 1000 (the default value to use) then I'm just using 1000 to save on run-time.

Run all samples:
```{python, eval=FALSE}
import os
import math

fp = 'cat_reads/'
cat_reads = sorted(os.listdir(fp))

for fq in cat_reads:
  count = len(open(fp+fq).readlines())/4
  sample = int(math.floor(count/100))
  if sample > 1000: sample = 1000
  np = 'nonpareil -s '+fp+fq+' -T kmer -f fastq -b '+'nonpareil/'+fq.split('.')[0]+' -X '+str(sample)
  os.system(np)
```

O3-pos:
```{python, eval=False}
import os
import math

fp = 'cat_reads/'
cat_reads = sorted(os.listdir(fp))

for fq in cat_reads:
  if fq != 'O3-pos.fastq': continue
  count = len(open(fp+fq).readlines())/4
  sample = 1000
  np = 'nonpareil -s '+fp+fq+' -T kmer -f fastq -b '+'nonpareil/'+fq.split('.')[0]+' -X '+str(sample)
  os.system(np)
```

## HUMAnN3

Run on all samples (5/3/21):
```{bash, eval=FALSE}
parallel -j 1 'humann --input {} --output humann3_out/ --threads 12 --nucleotide-database /home/shared/HUMANN/chocophlan/ --protein-database /home/shared/HUMANN/uniref/' ::: cat_reads/*
```

Try again O3-pos (it was the fastq that was a problem and this was run after the fasta file so it overwrote the output):
```{bash, eval=FALSE}
parallel -j 1 'humann --input {} --output humann3_out/ --threads 12 --nucleotide-database /home/shared/HUMANN/chocophlan/ --protein-database /home/shared/HUMANN/uniref/' ::: cat_reads/O3-pos.fasta
```

Move bugs lists and merge (the merge isn't working for some reason, but there are very few classifications anyway so I'm leaving this):
```{bash, eval=FALSE}
mkdir humann3_out/bugs_lists/
for i in humann3_out/* ; do echo $i/*_bugs_list.tsv ; done
humann3_out/bugs_lists/ ; done
merge_metaphlan_tables.py humann3_out/bugs_lists/*tsv > humann3_out/metaphlan_merged.tsv
```

Merge other tables:
```{bash, eval=False}
humann_join_tables -s --input humann3_out/ --file_name pathabundance --output humann3_out/humann3_pathabundance.tsv
humann_join_tables -s --input humann3_out/ --file_name pathcoverage --output humann3_out/humann3_pathcoverage.tsv
humann_join_tables -s --input humann3_out/ --file_name genefamilies --output humann3_out/humann3_genefamilies.tsv
```

Renormalise files:
```{bash, eval=FALSE}
humann_renorm_table --input humann3_pathabundance.tsv --units relab --output humann3_pathabundance_relab.tsv
humann_split_stratified_table --input humann3_pathabundance_relab.tsv --output ./

humann_renorm_table --input humann3_genefamilies.tsv --units relab --output humann3_genefamilies_relab.tsv
humann_split_stratified_table --input humann3_genefamilies_relab.tsv --output ./
```

Remove other intermediates :
```{bash, eval=FALSE}
find . -name \*aligned.sam -type f -delete
find . -name \*unaligned.fa -type f -delete
find . -name \*aligned.tsv -type f -delete
```

Renormalise gene numbers (taking into account gene length):
```{bash, eval=FALSE}
humann_renorm_table --input humann3_genefamilies.tsv --output humann3_genefamilies_cpm.tsv --units cpm --update-snames
```

# Composition

## Standard Kraken database (i.e. including SARS-CoV-2)

Read in kraken:
```{python, results='hide', fig.keep='all'}
name = 'kraken_standard_Nov20'
results_folder = folder+'/metagenome/'+name+'/kraken2_kreport/'
save_folder = folder+'metagenome/'

with open(save_folder+name+'.dict', 'rb') as f:
  taxid = pickle.load(f)

bracken = os.listdir(results_folder)
bracken = [f for f in bracken if '.bracken' in f]
tax_names = {}

first = True

for sample in bracken:
  sn = sample
  # if sample != 'cDNA-N1-neg.bracken': continue
  sample = pd.read_csv(results_folder+sample, header=0, sep='\t')
  keeping = []
  for row in sample.index.values:
    if sample.loc[row, 'taxonomy_id'] in taxid:
      keeping.append(True)
      if sample.loc[row, 'taxonomy_id'] not in tax_names:
        tax_names[sample.loc[row, 'taxonomy_id']] = sample.loc[row, 'name']
    else:
      keeping.append(False)
  sample = pd.DataFrame(sample.loc[keeping, :].set_index('taxonomy_id').loc[:, 'new_est_reads'])
  # sample = sample
  # sample = pd.DataFrame(sample)
  if first:
    all_samples = sample.rename(columns={'new_est_reads':sn.split('.')[0]})
    first = False
  else:
    sample = sample.rename(columns={'new_est_reads':sn.split('.')[0]})
    all_samples = pd.concat([all_samples, sample]).fillna(value=0)
all_samples_kraken_stan = all_samples.groupby(by=all_samples.index, axis=0).sum()
```

QUAST results:
```{python, results='hide', fig.keep='all'}
quast_folder = folder+'/metagenome/quast/quast_results/summaries_separate_kraken/'
quast = os.listdir(quast_folder)
quast = sorted([f for f in quast if name in f])
file_types = ['Genome_fraction', 'Largest_alignment', 'Largest_contig', 'num_contigs']
dfs = [[], [], [], []]
df_added = [True, True, True, True]

for sample in all_samples_kraken_stan.columns:
  for fn in quast:
    if sample in fn:
      ftype = fn.split('.')[0].split('_')
      ftype = ftype[-2]+'_'+ftype[-1]
      if ftype == '100_report': continue
      ind = -1
      for a in range(len(file_types)):
        if file_types[a] == ftype: ind = a
      quast_result = pd.read_csv(quast_folder+fn, index_col=0, header=0, sep='\t').transpose().rename(columns={101564:sample}).rename(columns={'101564':sample})
      quast_result = quast_result.rename(index={quast_result.index.values[0]:'101564'})
      quast_result = pd.DataFrame(quast_result.loc[:, sample])
      if df_added[ind]:
        new_df = pd.DataFrame(quast_result)
        df_added[ind] = False
        dfs[ind] = new_df
      else:
        new_df = pd.concat([dfs[ind], quast_result]).fillna(value=0)
        dfs[ind] = new_df

for d in range(len(dfs)):
  df = dfs[d].groupby(by=dfs[d].index, axis=0).sum().fillna(value=0)
  df.index = df.index.map(str)
  df.columns = df.columns.astype(str)
  dfs[d] = df

dfs_stan = list(dfs)
for d in range(len(dfs_stan)):
  dfs_stan[d] = pd.DataFrame(dfs_stan[d])
```

Plot correlation:
```{python, results='hide', fig.keep='all'}
all_samples_kraken = all_samples_kraken_stan
all_samples_kraken.index = all_samples_kraken.index.map(str) 
all_samples_kraken.columns = all_samples_kraken.columns.astype(str) 
kraken_perc = all_samples_kraken.divide(all_samples_kraken.sum(axis=0), axis=1).multiply(100)

# samples = sorted(kraken_perc.columns)
samples = ['N1-neg', 'N2-neg', 'N3-neg', 'N4-neg', 'N5-neg', 'N1-pos', 'N2-pos', 'N3-pos', 'N4-pos', 'N5-pos', 'O1-neg', 'O2-neg', 'O3-neg', 'O4-neg', 'O5-neg', 'O1-pos', 'O2-pos', 'O3-pos', 'O4-pos', 'O5-pos', 'cDNA-N1-neg', 'cDNA-N2-neg', 'cDNA-N3-neg', 'cDNA-N4-neg', 'cDNA-N5-neg', 'cDNA-N1-pos', 'cDNA-N2-pos', 'cDNA-N3-pos', 'cDNA-N4-pos', 'cDNA-N5-pos', 'cDNA-O1-neg', 'cDNA-O2-neg', 'cDNA-O3-neg', 'cDNA-O4-neg', 'cDNA-O5-neg', 'cDNA-O1-pos', 'cDNA-O2-pos', 'cDNA-O3-pos', 'cDNA-O4-pos', 'cDNA-O5-pos']

dfs = dfs_stan
df_log = dfs[1]
df_log[df_log == 0] = 0.01
df_log = np.log10(df_log)
mi, ma = min(df_log.min()), max(df_log.max())
colormap = mpl.cm.get_cmap('plasma', 256)
norm = mpl.colors.Normalize(vmin=mi, vmax=ma)
m = mpl.cm.ScalarMappable(norm=norm, cmap=colormap)

fig = plt.figure(figsize=(20,10))
subplots = fig.subplots(4, 10)
a, b = 0, 0
for sample in samples:
  ax = plt.sca(subplots[a, b])
  x, y = [], []
  la, lc, nc = [], [], []
  for row in kraken_perc.index.values:
    reads = kraken_perc.loc[row, sample]
    gen_frac = dfs[0].loc[row, sample]
    large_align = df_log.loc[row, sample]
    large_contig = dfs[2].loc[row, sample]
    num_contig = dfs[3].loc[row, sample]
    x.append(reads), y.append(gen_frac)
    la.append(large_align), lc.append(large_contig), nc.append(num_contig)
    plt.scatter(np.log10(reads), np.log10(gen_frac), marker='o', s=num_contig*2, color=m.to_rgba(large_align), edgecolor='gray', lw=1)
    #plt.scatter(reads, gen_frac, marker='o', s=num_contig, color=m.to_rgba(large_align), edgecolor='gray', lw=1)
  #plt.semilogy(), plt.semilogx()
  #plt.xlim([pow(10, 0.01), pow(10, max(x))]), plt.ylim([pow(10, 0.01), pow(10, max(y))])
  plt.yticks(fontsize=8), plt.xticks(fontsize=8)
  plt.title(sample, fontsize=10)
  b += 1
  if b > 9:
    a += 1
    b = 0
fig.text(0, 0.5, 'Log10(Genome fraction (%))', rotation=90, va='center', ha='left')
fig.text(0.5, 0, 'Log10(Relative abundance (%))', va='bottom', ha='center')
plt.tight_layout()
plt.show()
plt.savefig(save_folder+'Reads vs genome fraction standard.png', dpi=600, bbox_inches='tight')
```

## Kraken RefSeq Complete

Read in kraken:
```{python, results='hide', fig.keep='all'}
name = 'kraken_RSCV93'
results_folder = folder+'/metagenome/'+name+'/kraken2_kreport/'
save_folder = folder+'metagenome/'

with open(save_folder+name+'.dict', 'rb') as f:
  taxid = pickle.load(f)

bracken = os.listdir(results_folder)
bracken = [f for f in bracken if '.bracken' in f]
tax_names = {}

first = True

for sample in bracken:
  sn = sample
  # if sample != 'cDNA-N1-neg.bracken': continue
  sample = pd.read_csv(results_folder+sample, header=0, sep='\t')
  keeping = []
  for row in sample.index.values:
    if sample.loc[row, 'taxonomy_id'] in taxid:
      keeping.append(True)
      if sample.loc[row, 'taxonomy_id'] not in tax_names:
        tax_names[sample.loc[row, 'taxonomy_id']] = sample.loc[row, 'name']
    else:
      keeping.append(False)
  sample = pd.DataFrame(sample.loc[keeping, :].set_index('taxonomy_id').loc[:, 'new_est_reads'])
  # sample = sample
  # sample = pd.DataFrame(sample)
  if first:
    all_samples = sample.rename(columns={'new_est_reads':sn.split('.')[0]})
    first = False
  else:
    sample = sample.rename(columns={'new_est_reads':sn.split('.')[0]})
    all_samples = pd.concat([all_samples, sample]).fillna(value=0)
all_samples_kraken_RSC = all_samples.groupby(by=all_samples.index, axis=0).sum()
```

QUAST results:
```{python, results='hide', fig.keep='all'}
quast_folder = folder+'/metagenome/quast/quast_results/summaries_separate_kraken/'
quast = os.listdir(quast_folder)
quast = sorted([f for f in quast if name in f])
file_types = ['Genome_fraction', 'Largest_alignment', 'Largest_contig', 'num_contigs']
dfs = [[], [], [], []]
df_added = [True, True, True, True]

for sample in all_samples_kraken_RSC.columns:
  #if sample != 'N3_neg': continue
  for fn in quast:
    if sample in fn:
      ftype = fn.split('.')[0].split('_')
      ftype = ftype[-2]+'_'+ftype[-1]
      if ftype == '100_report': continue
      ind = -1
      for a in range(len(file_types)):
        if file_types[a] == ftype: ind = a
      quast_result = pd.read_csv(quast_folder+fn, index_col=0, header=0, sep='\t').transpose().rename(columns={1007104:sample}).rename(columns={'1007104':sample})
      quast_result = quast_result.rename(index={quast_result.index.values[0]:'1007104'})
      quast_result = pd.DataFrame(quast_result.loc[:, sample])
      if df_added[ind]:
        new_df = pd.DataFrame(quast_result)
        df_added[ind] = False
        dfs[ind] = new_df
      else:
        new_df = pd.concat([dfs[ind], quast_result]).fillna(value=0)
        dfs[ind] = new_df

for d in range(len(dfs)):
  df = dfs[d].groupby(by=dfs[d].index, axis=0).sum().fillna(value=0)
  df.index = df.index.map(str)
  df.columns = df.columns.astype(str)
  dfs[d] = df

dfs_RSC = list(dfs)
for d in range(len(dfs_RSC)):
  dfs_RSC[d] = pd.DataFrame(dfs_RSC[d])
```

Plot correlation:
```{python, results='hide', fig.keep='all'}
all_samples_kraken = all_samples_kraken_RSC
all_samples_kraken.index = all_samples_kraken.index.map(str) 
all_samples_kraken.columns = all_samples_kraken.columns.astype(str) 
kraken_perc = all_samples_kraken.divide(all_samples_kraken.sum(axis=0), axis=1).multiply(100)

# samples = sorted(kraken_perc.columns)
samples = ['N1-neg', 'N2-neg', 'N3-neg', 'N4-neg', 'N5-neg', 'N1-pos', 'N2-pos', 'N3-pos', 'N4-pos', 'N5-pos', 'O1-neg', 'O2-neg', 'O3-neg', 'O4-neg', 'O5-neg', 'O1-pos', 'O2-pos', 'O3-pos', 'O4-pos', 'O5-pos', 'cDNA-N1-neg', 'cDNA-N2-neg', 'cDNA-N3-neg', 'cDNA-N4-neg', 'cDNA-N5-neg', 'cDNA-N1-pos', 'cDNA-N2-pos', 'cDNA-N3-pos', 'cDNA-N4-pos', 'cDNA-N5-pos', 'cDNA-O1-neg', 'cDNA-O2-neg', 'cDNA-O3-neg', 'cDNA-O4-neg', 'cDNA-O5-neg', 'cDNA-O1-pos', 'cDNA-O2-pos', 'cDNA-O3-pos', 'cDNA-O4-pos', 'cDNA-O5-pos']

dfs = dfs_RSC
df_log = dfs[1]
df_log[df_log == 0] = 0.01
df_log = np.log10(df_log)
mi, ma = min(df_log.min()), max(df_log.max())
colormap = mpl.cm.get_cmap('plasma', 256)
norm = mpl.colors.Normalize(vmin=mi, vmax=ma)
m = mpl.cm.ScalarMappable(norm=norm, cmap=colormap)

fig = plt.figure(figsize=(20,10))
subplots = fig.subplots(4, 10)
a, b = 0, 0
for sample in samples:
  ax = plt.sca(subplots[a, b])
  x, y = [], []
  la, lc, nc = [], [], []
  for row in kraken_perc.index.values:
    reads = kraken_perc.loc[row, sample]
    gen_frac = dfs[0].loc[row, sample]
    large_align = df_log.loc[row, sample]
    large_contig = dfs[2].loc[row, sample]
    num_contig = dfs[3].loc[row, sample]
    x.append(reads), y.append(gen_frac)
    la.append(large_align), lc.append(large_contig), nc.append(num_contig)
    plt.scatter(np.log10(reads), np.log10(gen_frac), marker='o', s=num_contig*2, color=m.to_rgba(large_align), edgecolor='gray', lw=1)
    #plt.scatter(reads, gen_frac, marker='o', s=num_contig, color=m.to_rgba(large_align), edgecolor='gray', lw=1)
  #plt.semilogy(), plt.semilogx()
  #plt.xlim([pow(10, 0.01), pow(10, max(x))]), plt.ylim([pow(10, 0.01), pow(10, max(y))])
  plt.yticks(fontsize=8), plt.xticks(fontsize=8)
  plt.title(sample, fontsize=10)
  b += 1
  if b > 9:
    a += 1
    b = 0
fig.text(0, 0.5, 'Log10(Genome fraction (%))', rotation=90, va='center', ha='left')
fig.text(0.5, 0, 'Log10(Relative abundance (%))', va='bottom', ha='center')
plt.tight_layout()
plt.show()
plt.savefig(save_folder+'Reads vs genome fraction RSCV93.png', dpi=600, bbox_inches='tight')
```

## Correlations

```{python, results='hide', fig.keep='all'}
# samples.reverse()
# dfs_RSC, all_samples_kraken_RSC
# dfs_stan, all_samples_kraken_stan
all_samples_kraken_RSC_perc = all_samples_kraken_RSC.divide(all_samples_kraken_RSC.sum(axis=0), axis=1).multiply(100)
all_samples_kraken_stan_perc = all_samples_kraken_stan.divide(all_samples_kraken_stan.sum(axis=0), axis=1).multiply(100)

plt.figure(figsize=(10,20))
ax1 = plt.subplot2grid((1,20), (0,0), frameon=False, colspan=18)
ax_cbar = plt.subplot2grid((5,20), (0,19))

colormap = mpl.cm.get_cmap('plasma', 256)
norm = mpl.colors.Normalize(vmin=-0.3, vmax=0.3)
m = mpl.cm.ScalarMappable(norm=norm, cmap=colormap)
count = 0
y = []
all_corr = []

for sample in samples:
  reads_RSC, reads_stan, perc_RSC, perc_stan, gf_RSC, gf_stan, la_RSC, la_stan, lc_RSC, lc_stan, nc_RSC, nc_stan = [], [], [], [], [], [], [], [], [], [], [], []
  this_corr = []
  for row in all_samples_kraken_RSC.index.values:
    reads_RSC.append(all_samples_kraken_RSC.loc[row, sample])
    perc_RSC.append(all_samples_kraken_RSC_perc.loc[row, sample])
    gf_RSC.append(dfs_RSC[0].loc[row, sample])
    la_RSC.append(dfs_RSC[1].loc[row, sample])
    lc_RSC.append(dfs_RSC[2].loc[row, sample])
    nc_RSC.append(dfs_RSC[3].loc[row, sample])
  for row in all_samples_kraken_stan.index.values:
    reads_stan.append(all_samples_kraken_stan.loc[row, sample])
    perc_stan.append(all_samples_kraken_stan_perc.loc[row, sample])
    gf_stan.append(dfs_stan[0].loc[row, sample])
    la_stan.append(dfs_stan[1].loc[row, sample])
    lc_stan.append(dfs_stan[2].loc[row, sample])
    nc_stan.append(dfs_stan[3].loc[row, sample])
  x = [[0, 1, 2, 3], [5, 6, 7, 8], [10, 11, 12, 13], [15, 16, 17, 18]]
  comps = [[gf_RSC, la_RSC, lc_RSC, nc_RSC], [gf_RSC, la_RSC, lc_RSC, nc_RSC], [gf_stan, la_stan, lc_stan, nc_stan], [gf_stan, la_stan, lc_stan, nc_stan]]
  comp_to = [reads_RSC, perc_RSC, reads_stan, perc_stan]
  for a in range(len(comps)):
    for b in range(len(comps[a])):
      # corr, p = pearsonr(comps[a][b], comp_to[a])
      this_comp, this_comp_to = comps[a][b], comp_to[a]
      this_comp = [c+1 for c in this_comp]
      this_comp_to = [c+1 for c in this_comp_to]
      corr, p = pearsonr(np.log10(this_comp), np.log10(this_comp_to))
      # corr, p = spearmanr(np.log10(comps[a][b]), np.log10(comp_to[a]))
      # corr, p = spearmanr(comps[a][b], comp_to[a])
      this_corr.append(corr)
      col = m.to_rgba(corr)
      ax1.bar(x[a][b], 1, bottom=count, width=1, color=col, edgecolor='k', )
      if p < 0.05:
        ax1.text(x[a][b], count+0.5, '*', ha='center', va='center')

  y.append(count+0.5)
  count += 1
  all_corr.append(corr)

cb1 = mpl.colorbar.ColorbarBase(ax_cbar, cmap=colormap, norm=norm, orientation='vertical')
plt.sca(ax1)
ax1.set_ylim([0, 40]), ax1.set_xlim([-0.5, 18.6])
plt.yticks(y, samples)
x = x[0]+x[1]+x[2]+x[3]
plt.xticks(x, ['Genome fraction', 'Largest alignment', 'Largest contig', 'Number of contigs', 'Genome fraction', 'Largest alignment', 'Largest contig', 'Number of contigs', 'Genome fraction', 'Largest alignment', 'Largest contig', 'Number of contigs', 'Genome fraction', 'Largest alignment', 'Largest contig', 'Number of contigs', ], rotation=90)
ax1.xaxis.tick_top()
x = [1.5, 6.5, 11.5, 16.5]
name = ['RefSeq Complete\nReads $vs$', 'RefSeq Complete\nRelative abundance $vs$', 'Standard\nReads $vs$', 'Standard\nRelative abundance $vs$']
for a in range(len(x)):
  ax1.text(x[a], 44, name[a], ha='center', va='bottom', fontweight='bold')
plt.show()   
plt.savefig(save_folder+'Correlation pearsons log.png', dpi=600, bbox_inches='tight')
print(all_corr)
```



```{python}
#print(reads_RSC, perc_RSC)
```


