---
title: "PGPC blood microbiome Anvio"
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
  pdf_document:
    toc: yes
---

```{R, results='hide', fig.keep='all', message=FALSE, include=FALSE, eval=FALSE}
library(reticulate)
#library(kableExtra)
library(knitr)
library(phyloseq)
```

```{python, results='hide', fig.keep='all', message=FALSE, include=FALSE}
import numpy as np
import os
import pandas as pd
import math
import matplotlib.pyplot as plt
from scipy.cluster import hierarchy
import matplotlib as mpl
from matplotlib_venn import venn2
import csv
from matplotlib.patches import Patch
import pickle
from scipy.spatial import distance
from scipy import stats
from sklearn import manifold
from sklearn.decomposition import PCA

save_path = '/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/Human_blood_metagenome/analysis/output/'
analysis = '/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/Human_blood_metagenome/analysis/'
```

Gavins assembly commands:
```{bash, eval=FALSE}
# Working in /home/gavin/projects/primate_microbiome/mgs_datasets/amato2019_PE

conda deactivate
conda activate anvio-6.2

R1=$( ls bbmap_out/*_1.fastq.gz | tr '\n' ',' | sed 's/,$//' )
R2=$( ls bbmap_out/*_2.fastq.gz | tr '\n' ',' | sed 's/,$//'  )
    
 megahit -1 $R1 \
         -2 $R2 \
         --min-contig-len 1000 \
         --num-cpu-threads 35 \
         --presets meta-large \
         --memory 0.8 \
         -o megahit_out \
        --verbose
    

# Remove intermediate contigs to save space
rm -r megahit_out_by_set/*/intermediate_contigs

# This step ensured that all characters used in the scaffold names were compatible with the downstream anvi’o analyses.
# Also removed scaffolds too short to produce a reliable tetra-nucleotide frequency (1000 bp)
# After that then also created database of these contigs, which will be output to new folder ("contig_databases")

anvi-script-reformat-fasta megahit_out/final.contigs.fa \
                               --simplify-names \
                               --min-len 1000 \
                               -o megahit_out/final.contigs.fixed.fa \
                               --prefix Abx

mkdir anvio_databases

anvi-gen-contigs-database -f megahit_out/final.contigs.fixed.fa \
                              -o anvio_databases/CONTIGS.db \
                              -n Abx


mkdir anvio_databases

anvi-script-reformat-fasta megahit_out/final.contigs.fixed.fa \
                           --min-len 1000 \
                           --simplify-names \
                           -o megahit_out/final.contigs.fixed.min1000.fa
    
anvi-gen-contigs-database -f megahit_out/final.contigs.fixed.min1000.fa \
                              -o anvio_databases/CONTIGS.db \
                              -n Abx

anvi-run-hmms -c anvio_databases/CONTIGS.db --num-threads 20



anvi-get-sequences-for-gene-calls -c anvio_databases/CONTIGS.db -o anvio_databases/gene_calls.fa
        
kaiju -t /scratch/db/kaiju_db_nr_euk/nodes.dmp \
      -f /scratch/db/kaiju_db_nr_euk/kaiju_db_nr_euk.fmi \
      -i anvio_databases/gene_calls.fa \
      -o anvio_databases/gene_calls.nr.out \
      -z 10 \
      -v
    
kaiju-addTaxonNames -t /scratch/db/kaiju_db_nr_euk/nodes.dmp \
              -n /scratch/db/kaiju_db_nr_euk/names.dmp \
              -i anvio_databases/gene_calls.nr.out \
              -o anvio_databases/gene_calls.nr.names \
              -r superkingdom,phylum,class,order,family,genus,species

anvi-import-taxonomy-for-genes -i anvio_databases/gene_calls.nr.names \
                               -c anvio_databases/CONTIGS.db \
                               -p kaiju \
                               --just-do-it



bowtie2-build megahit_out/final.contigs.fixed.min1000.fa megahit_out/final.contigs.fixed.min1000


mkdir bam_files

for SAMPLE in `awk '{print $1}' sample_ids.txt`
do

    # do the bowtie mapping to get the SAM file:
    bowtie2 --threads 30 \
            -x megahit_out/final.contigs.fixed.min1000 \
            -1 "raw_data/"$SAMPLE"_1.fastq.gz" \
            -2 "raw_data/"$SAMPLE"_2.fastq.gz" \
            --no-unal \
            -S bam_files/$SAMPLE.sam

    # covert the resulting SAM file to a BAM file:
    samtools view -F 4 -bS bam_files/$SAMPLE.sam > bam_files/$SAMPLE-RAW.bam

    # sort and index the BAM file:
    samtools sort bam_files/$SAMPLE-RAW.bam -o bam_files/$SAMPLE.bam
    samtools index bam_files/$SAMPLE.bam

    # remove temporary files:
    rm bam_files/$SAMPLE.sam bam_files/$SAMPLE-RAW.bam

done


# Generate anvi’o profile databases that contain the coverage and detection statistics of each scaffold
mkdir anvio_databases/profiles

for SAMPLE in `awk '{print $1}' sample_ids.txt`
do

    anvi-profile -c anvio_databases/CONTIGS.db \
                 -i bam_files/$SAMPLE.bam \
                 --num-threads 20 \
                 -o anvio_databases/profiles/$SAMPLE
done




#### STOPPED HERE BECAUSE THE NEXT STEP WAS TAKING > 2 DAYS AND ~35% MEMORY ON VULCAN... SHOULD WAIT FOR A MORE CONVENIENT TIME TO RUN THIS
# Merge sample profiles
anvi-merge -c anvio_databases/CONTIGS.db \
           -o anvio_databases/merged_profiles \
           anvio_databases/profiles/ERR7*/PROFILE.db

for SAMPLE in `awk '{print $1}' sample_ids.txt`
do
    anvi-cluster-contigs -c anvio_databases/CONTIGS.db \
                         -p anvio_databases/profiles/$SAMPLE/PROFILE.db \
                         --collection-name $SAMPLE"_concoct" \
                         --driver concoct \
                         --num-threads 10 \
                         --just-do-it
done

mkdir concoct_summary

for SET in `cat primate_sets.txt`
do
    anvi-summarize -c contig_databases/$SET/$SET-CONTIGS.db \
                   -p profile_databases/$SET-MERGED/PROFILE.db \
                   -C $SET"_concoct" \
                   -o concoct_summary/$SET-summary
done



anvi-estimate-genome-completeness -c contig_databases/$SET/$SET-CONTIGS.db \
                                  -p profile_databases/$SET-MERGED/PROFILE.db \
                                  -C $SET"_concoct"

anvi-refine -c contig_databases/$SET/$SET-CONTIGS.db \
            -p profile_databases/$SET-MERGED/PROFILE.db \
            -C $SET"_concoct" \
            -b Bin_136 --server-only -P 8082

mkdir hmm_hits

for SET in `cat primate_sets.txt`
do
anvi-get-sequences-for-hmm-hits -c contig_databases/$SET/$SET-CONTIGS.db \
                                -p profile_databases/$SET-MERGED/PROFILE.db \
                                -C $SET"_concoct" \
                                 -o hmm_hits/$SET"_concat-proteins.fa" \
                                --hmm-source 'Bacteria_71' \
                                --return-best-hit \
                                --get-aa-sequences \
                                --concatenate
done


# For each SET parse the output and retain only bins with completeness >90% and redundancy < 10%. Re-name seqs in FASTA to be "SET_bin#" and remove all gaps (so that re-alignment can be done) 
mkdir hmm_hits_clean

for SET in `cat primate_sets.txt`
do
python /home/gavin/projects/POMS/scripts/parse_anvio_faa_output.py -f hmm_hits/$SET"_concat-proteins.fa" \
                                                                   -s concoct_summary/$SET-summary/bins_summary.txt \
                                                                   -d $SET \
                                                                   -o hmm_hits_clean/$SET"_concat-proteins_clean.fa"
done

# Concatenate bins across all sets
cat hmm_hits_clean/*fa > ALL-SETS_concat-proteins_clean.fa

# Align with muscle (with all default options)
muscle3.8.31_i86linux64 -in ALL-SETS_concat-proteins_clean.fa -out ALL-SETS_concat-proteins_clean_aligned.fa

# Run fasttree
anvi-gen-phylogenomic-tree -f ALL-SETS_concat-proteins_clean_aligned.fa \
                           -o ALL-SETS_concat-proteins_clean_aligned.tre                           


### Annotate MAGs with KOs.
# First need to extract protein sequences
mkdir protein_seqs

for SET in `cat primate_sets.txt`
do
    anvi-get-sequences-for-gene-calls -c contig_databases/$SET/$SET-CONTIGS.db \
                                      --get-aa-sequences \
                                      -o protein_seqs/$SET-protein_seqs.faa
done

# Give each protein different id per SET
# Note that the "genecall" part of this was removed later, so that part should be removed in the future.
for SET in `cat primate_sets.txt`
do
    REPLACE=">genecall-$SET""_"
    echo sed -i \'"s/>/$REPLACE/"\' protein_seqs/$SET-protein_seqs.faa
done

cat /home/gavin/projects/primate_microbiome/mgs_datasets/amato2019_PE/protein_seqs/*faa > /home/gavin/projects/primate_microbiome/mgs_datasets/amato2019_PE/all_protein_seqs.faa

sed -i 's/genecall-//' all_protein_seqs.faa

# Temporarily move to kofam_scan dir to run executable:
cd /home/gavin/local/prg/kofam_scan/

./exec_annotation /home/gavin/projects/primate_microbiome/mgs_datasets/amato2019_PE/all_protein_seqs.faa \
                  -o /home/gavin/projects/primate_microbiome/mgs_datasets/amato2019_PE/all_protein_seqs_kofamscan.txt \
                  --profile /scratch/db/kofamscan_db/profiles/ \
                  --ko-list /scratch/db/kofamscan_db/ko_list \
                  --cpu 20 \
                  --format detail-tsv \
                  --report-unannotated

# Parse sig hits based on score threshold (indicated by *) into separate file:
grep -e "^*" /home/gavin/projects/primate_microbiome/mgs_datasets/amato2019_PE/all_protein_seqs_kofamscan.txt | awk '{ print $2"\t"$3 }' > /home/gavin/projects/primate_microbiome/mgs_datasets/amato2019_PE/all_protein_seqs_kofamscan_hits.txt

           
cd /home/gavin/projects/primate_microbiome/mgs_datasets/amato2019_PE/


rm concoct_summary/bin_gene_calls.tsv

for SET in `cat primate_sets.txt`
do
    for BIN in $(ls concoct_summary/$SET-summary/bin_by_bin)
    do
      tail -n +2 concoct_summary/$SET-summary/bin_by_bin/$BIN/$BIN-gene_calls.txt | awk -v set=$SET -v bin="$BIN" '{print set "_" bin "\t" set "_" $1}' >> concoct_summary/bin_gene_calls.tsv
    done
done


# Get table of KOs per MAG
python /home/gavin/projects/POMS/scripts/link_long_tables.py --file1 concoct_summary/bin_gene_calls.tsv \
                                                             --file2 all_protein_seqs_kofamscan_hits.txt \
                                                             --output Amato2019_MAG_genes.tsv
```

Modified for my samples:
```{bash, eval=FALSE}
conda deactivate
conda activate anvio-6.2

R1=$( ls joined_lanes_separate_reads/*_R1.fastq.gz | tr '\n' ',' | sed 's/,$//' )
R2=$( ls joined_lanes_separate_reads/*_R2.fastq.gz | tr '\n' ',' | sed 's/,$//'  )
    
 megahit -1 $R1 \
         -2 $R2 \
         --min-contig-len 1000 \
         --num-cpu-threads 12 \
         --presets meta-large \
         --memory 0.3 \
         -o anvio/megahit_out \
        --verbose

# Remove intermediate contigs to save space
rm -r megahit_out_by_set/*/intermediate_contigs

# This step ensured that all characters used in the scaffold names were compatible with the downstream anvi’o analyses.
# Also removed scaffolds too short to produce a reliable tetra-nucleotide frequency (1000 bp)
# After that then also created database of these contigs, which will be output to new folder ("contig_databases")

anvi-script-reformat-fasta megahit_out/final.contigs.fa \
                               --simplify-names \
                               --min-len 1000 \
                               -o megahit_out/final.contigs.fixed.fa \
                               --prefix blood

mkdir anvio_databases

anvi-gen-contigs-database -f megahit_out/final.contigs.fixed.fa \
                              -o anvio_databases/CONTIGS.db \
                              -n blood

anvi-run-hmms -c anvio_databases/CONTIGS.db --num-threads 12

anvi-get-sequences-for-gene-calls -c anvio_databases/CONTIGS.db -o anvio_databases/gene_calls.fa

# install kaiju
# conda install -c bioconda kaiju
        
kaiju -t /scratch/db/kaiju_db_nr_euk/nodes.dmp \
      -f /scratch/db/kaiju_db_nr_euk/kaiju_db_nr_euk.fmi \
      -i anvio_databases/gene_calls.fa \
      -o anvio_databases/gene_calls.nr.out \
      -z 10 \
      -v
    
kaiju-addTaxonNames -t /scratch/db/kaiju_db_nr_euk/nodes.dmp \
              -n /scratch/db/kaiju_db_nr_euk/names.dmp \
              -i anvio_databases/gene_calls.nr.out \
              -o anvio_databases/gene_calls.nr.names \
              -r superkingdom,phylum,class,order,family,genus,species

anvi-import-taxonomy-for-genes -i anvio_databases/gene_calls.nr.names \
                               -c anvio_databases/CONTIGS.db \
                               -p kaiju \
                               --just-do-it



bowtie2-build megahit_out/final.contigs.fixed.min1000.fa megahit_out/final.contigs.fixed.min1000


mkdir bam_files
cd ..

#I need to make this sample_ids.txt file
python
import os

files = os.listdir('joined_lanes_separate_reads/')
files = [f for f in files if 'R1' in f]
files = [f.split('_') for f in files]
files = [f[0]+'_'+f[1] for f in files]
files = sorted(files)
with open('sample_ids.txt', 'w') as f:
  for sample in files:
    f.write(sample+'\n')

# Now do mapping
for SAMPLE in `awk '{print $1}' sample_ids.txt`
do

    # do the bowtie mapping to get the SAM file:
    bowtie2 --threads 12 \
            -x anvio/megahit_out/final.contigs.fixed.min1000 \
            -1 "raw_reads_reformat/"$SAMPLE"_R1.fastq.gz" \
            -2 "raw_reads_reformat/"$SAMPLE"_R2.fastq.gz" \
            --no-unal \
            -S anvio/bam_files/$SAMPLE.sam

    # covert the resulting SAM file to a BAM file:
    samtools view -F 4 -bS anvio/bam_files/$SAMPLE.sam > anvio/bam_files/$SAMPLE-RAW.bam

    # sort and index the BAM file:
    samtools sort anvio/bam_files/$SAMPLE-RAW.bam -o anvio/bam_files/$SAMPLE.bam
    samtools index anvio/bam_files/$SAMPLE.bam

    # remove temporary files:
    rm anvio/bam_files/$SAMPLE.sam anvio/bam_files/$SAMPLE-RAW.bam

done

mv sample_ids.txt anvio/
cd anvio

# Generate anvi’o profile databases that contain the coverage and detection statistics of each scaffold
mkdir anvio_databases/profiles

for SAMPLE in `awk '{print $1}' sample_ids.txt`
do

    anvi-profile -c anvio_databases/CONTIGS.db \
                 -i bam_files/$SAMPLE.bam \
                 --num-threads 12 \
                 -o anvio_databases/profiles/$SAMPLE
done

Getting this output:
WARNING
=====================================
According to the data generated in the contigs database, there are 12666 contigs
in your BAM file with 0 gene calls. Which may not be unusual if (a) some of your
contigs are very short, or (b) your the gene caller was not capable of dealing
with the type of data you had. If you would like to take a look yourself, here
is one contig that is missing any genes: c_000000009922



Config Error: At least one contig name in your BAM file does not match contig names stored in
              the contigs database. For instance, this is one contig name found in your BAM  
              file: 'c_000000000001', and this is another one found in your contigs database:
              'blood_000000003939'. You may be using an contigs database for profiling that  
              has nothing to do with the BAM file you are trying to profile, or you may have 
              failed to fix your contig names in your FASTA file prior to mapping, which is  
              described here: http://goo.gl/Q9ChpS 

# So I am going to edit the headers of the bam files from c to blood (as in the other files)
for SAMPLE in `awk '{print $1}' sample_ids.txt`
do
     samtools view -H $SAMPLE".bam" > header.sam
     sed "s/c/blood/" header.sam > header_corrected.sam
     samtools reheader header_corrected.sam $SAMPLE".bam" > $SAMPLE"_new.bam"
     samtools index $SAMPLE"_new.bam"
done

#And redo the previous step
for SAMPLE in `awk '{print $1}' sample_ids.txt`
do

    anvi-profile -c anvio_databases/CONTIGS.db \
                 -i bam_files/$SAMPLE"_new.bam" \
                 --num-threads 12 \
                 -o anvio_databases/profiles/$SAMPLE
done


# Merge sample profiles
anvi-merge -c anvio_databases/CONTIGS.db \
           -o anvio_databases/merged_profiles \
           anvio_databases/profiles/PGPC*/PROFILE.db

anvi-cluster-contigs -c anvio_databases/CONTIGS.db \
                         -p anvio_databases/merged_profiles/PROFILE.db \
                         --collection-name "merged_concoct" \
                         --driver concoct \
                         --num-threads 12 \
                         --just-do-it

mkdir concoct_summary

anvi-summarize -c anvio_databases/CONTIGS.db \
                   -p anvio_databases/merged_profiles/PROFILE.db \
                   -C "merged_concoct" \
                   -o concoct_summary/$SET-summary
                  

anvi-estimate-genome-completeness -c anvio_databases/CONTIGS.db \
                                  -p anvio_databases/merged_profiles/PROFILE.db \
                                  -C "merged_concoct"
                                  
### STOPPED HERE FOR NOW ### 

anvi-refine -c contig_databases/$SET/$SET-CONTIGS.db \
            -p profile_databases/$SET-MERGED/PROFILE.db \
            -C $SET"_concoct" \
            -b Bin_136 --server-only -P 8082

mkdir hmm_hits

for SET in `cat primate_sets.txt`
do
anvi-get-sequences-for-hmm-hits -c contig_databases/$SET/$SET-CONTIGS.db \
                                -p profile_databases/$SET-MERGED/PROFILE.db \
                                -C $SET"_concoct" \
                                 -o hmm_hits/$SET"_concat-proteins.fa" \
                                --hmm-source 'Bacteria_71' \
                                --return-best-hit \
                                --get-aa-sequences \
                                --concatenate
done


# For each SET parse the output and retain only bins with completeness >90% and redundancy < 10%. Re-name seqs in FASTA to be "SET_bin#" and remove all gaps (so that re-alignment can be done) 
mkdir hmm_hits_clean

for SET in `cat primate_sets.txt`
do
python /home/gavin/projects/POMS/scripts/parse_anvio_faa_output.py -f hmm_hits/$SET"_concat-proteins.fa" \
                                                                   -s concoct_summary/$SET-summary/bins_summary.txt \
                                                                   -d $SET \
                                                                   -o hmm_hits_clean/$SET"_concat-proteins_clean.fa"
done

# Concatenate bins across all sets
cat hmm_hits_clean/*fa > ALL-SETS_concat-proteins_clean.fa

# Align with muscle (with all default options)
muscle3.8.31_i86linux64 -in ALL-SETS_concat-proteins_clean.fa -out ALL-SETS_concat-proteins_clean_aligned.fa

# Run fasttree
anvi-gen-phylogenomic-tree -f ALL-SETS_concat-proteins_clean_aligned.fa \
                           -o ALL-SETS_concat-proteins_clean_aligned.tre                           


### Annotate MAGs with KOs.
# First need to extract protein sequences
mkdir protein_seqs

for SET in `cat primate_sets.txt`
do
    anvi-get-sequences-for-gene-calls -c contig_databases/$SET/$SET-CONTIGS.db \
                                      --get-aa-sequences \
                                      -o protein_seqs/$SET-protein_seqs.faa
done

# Give each protein different id per SET
# Note that the "genecall" part of this was removed later, so that part should be removed in the future.
for SET in `cat primate_sets.txt`
do
    REPLACE=">genecall-$SET""_"
    echo sed -i \'"s/>/$REPLACE/"\' protein_seqs/$SET-protein_seqs.faa
done

cat /home/gavin/projects/primate_microbiome/mgs_datasets/amato2019_PE/protein_seqs/*faa > /home/gavin/projects/primate_microbiome/mgs_datasets/amato2019_PE/all_protein_seqs.faa

sed -i 's/genecall-//' all_protein_seqs.faa

# Temporarily move to kofam_scan dir to run executable:
cd /home/gavin/local/prg/kofam_scan/

./exec_annotation /home/gavin/projects/primate_microbiome/mgs_datasets/amato2019_PE/all_protein_seqs.faa \
                  -o /home/gavin/projects/primate_microbiome/mgs_datasets/amato2019_PE/all_protein_seqs_kofamscan.txt \
                  --profile /scratch/db/kofamscan_db/profiles/ \
                  --ko-list /scratch/db/kofamscan_db/ko_list \
                  --cpu 20 \
                  --format detail-tsv \
                  --report-unannotated

# Parse sig hits based on score threshold (indicated by *) into separate file:
grep -e "^*" /home/gavin/projects/primate_microbiome/mgs_datasets/amato2019_PE/all_protein_seqs_kofamscan.txt | awk '{ print $2"\t"$3 }' > /home/gavin/projects/primate_microbiome/mgs_datasets/amato2019_PE/all_protein_seqs_kofamscan_hits.txt

           
cd /home/gavin/projects/primate_microbiome/mgs_datasets/amato2019_PE/


rm concoct_summary/bin_gene_calls.tsv

for SET in `cat primate_sets.txt`
do
    for BIN in $(ls concoct_summary/$SET-summary/bin_by_bin)
    do
      tail -n +2 concoct_summary/$SET-summary/bin_by_bin/$BIN/$BIN-gene_calls.txt | awk -v set=$SET -v bin="$BIN" '{print set "_" bin "\t" set "_" $1}' >> concoct_summary/bin_gene_calls.tsv
    done
done


# Get table of KOs per MAG
python /home/gavin/projects/POMS/scripts/link_long_tables.py --file1 concoct_summary/bin_gene_calls.tsv \
                                                             --file2 all_protein_seqs_kofamscan_hits.txt \
                                                             --output Amato2019_MAG_genes.tsv
```